{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png) Filippo Miatto (2024) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: intro and tools\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### After going through this lecture you will be able to:\n",
    "1. Describe the mathematical representation of quantum states\n",
    "2. Use some advanced tensor tools from `numpy`\n",
    "3. Visualize qubits on the Bloch sphere\n",
    "4. Reason about wave functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Summary of Lecture 1:***\n",
    "_Today we introduce quantum **States**: the first of the three pillars of Quantum Mechanics (States, Transformations and Measurements). Quantum states are unit vectors in a complex vector space (Hilbert space) and they describe how a quantum system \"is\". As quantum states are vectors, they can be linearly combined to form other valid states, which are referred to as \"superpositions\".\n",
    "Qubits are the simplest possible quantum systems because their Hilbert space is only 2-dimensional, and therefore their states can be visualized on the Bloch sphere. More complex systems have states that are defined on higher-dimensional spaces, even infinite-dimensional spaces (in which case it makes more sense to talk about wavefunctions)._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "Quantum Mechanics is the theory that describes the most fundamental building blocks of everything. The typical systems that it applies to are atoms, particles, fields etc... and in principle it could apply to much larger systems. The difficulty with applying quantum mechanics to large systems is that the number of degrees of freedom involved grows linearly, but the complexity of the quantum mechanical description grows exponentially in the number of degrees of freedom. For this reason QM practically focuses on fundamental systems, but there is no theoretical limit to the size of the systems that it applies to.\n",
    "\n",
    "The only limit is our ability to use quantum mechanics, not its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. It's just linear algebra\n",
    "\n",
    "Quantum Mechanics is based on linear algebra. Whenever you feel you're struggling, you should remind yourself that \"it's just linear algebra!\".\n",
    "\n",
    "In QM we have three main classes of objects: States, Transformations and Measurements. All of them can be described by vectors, matrices, and/or higher order tensors. So no matter how complicated things may seem, it's all just linear algebra.\n",
    "\n",
    "In this first section we will set up a few important concepts and we will get you started with some useful functions and numpy tools.\n",
    "\n",
    "## 1.1 Complex vector spaces\n",
    "One important aspect of QM is that the vector spaces that we use are complex, i.e. the entries of the vectors are complex numbers. To help with clarity, we can indicate our complex vector space as $\\mathbb{C}^n$ where we have fixed its dimension to $n$. We will encounter vector spaces of various dimension such as $\\mathbb{C}^2$ (the Hilbert space of a qubit). Let's now see a couple of fundamental operations that we will often need in QM: inner products and norms.\n",
    "\n",
    "As our vector spaces are Hilbert spaces, they already come equipped with an inner product:\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{v}, \\mathbf{w}\\rangle = \\sum_iv_i^*w_i\n",
    "$$\n",
    "\n",
    "notice that the vector $\\bf v$ is conjugated when computing the inner product, i.e. we flip the sign of the imaginary component. \n",
    "\n",
    "Then the inner product can be used to define a norm (that is why all inner product spaces are also normed spaces):\n",
    "\n",
    "$$\n",
    "||\\mathbf{v}|| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v}\\rangle} = \\sqrt{\\sum_iv_i^*v_i}\n",
    "$$\n",
    "\n",
    "## 1.2 Dirac's notation\n",
    "There is a particular notation that has been adopted by the scientific community, known as Dirac's bra-ket notation. It comes from the alternative way of writing an inner product between two vectors $\\mathbf{v}$ and $\\mathbf{w}$ as $\\langle \\mathbf{v}|\\mathbf{w}\\rangle$. If you interpret it as two objects coming together and \"sticking\" to each other, you can separate them into $\\langle \\mathbf{v}|$ and $|\\mathbf{w}\\rangle$ and use them as row and column vectors (more appropriately as covectors and vectors). We call them \"bra\" and \"ket\" because together they sound like the word \"bracket\". Whatever we write inside the ket (or the bra) is just a label for that (co)vector, which sometimes carries useful information about the quantum state that it labels, like when we express the state of electromagnetic vacuum as $|0\\rangle$, meaning zero photons.\n",
    "\n",
    "As the vector space is complex, remember that to turn vectors and covectors into each other you also take the complex conjugate of the elements: $\\langle\\psi| = |\\psi\\rangle^\\dagger$ where the dagger symbol $\\dagger$ computes the \"hermitian conjugate\" (i.e. the conjugate transpose).\n",
    "\n",
    "We will make abundant use of inner products and norms. So let's begin our first activity by defining a couple of helper functions to compute inner products and norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1: inner products and norms\n",
    "- Write a function $f(\\mathbf{v}, \\mathbf{w})$ that computes the inner product between two vectors $\\mathbf{v}$ and $\\mathbf{w}$. The signature should be `f(array[complex], array[complex]) -> complex`\n",
    "- Write a function $f(\\mathbf{v})$ that computes the norm of a vector $\\mathbf{v}$. The signature should be `f(array[complex]) -> float`\n",
    "\n",
    "Note that we are not necessarily seeking optimal performance here, exactness is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.11.10)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inner_prod(v, w):\n",
    "    return np.sum(np.conj(v) * w)\n",
    "\n",
    "def norm(v):\n",
    "    return np.sqrt(inner_prod(v,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 A bit of useful numpy\n",
    "\n",
    "`Numpy` is one of the most popular python libraries for numerical calculus.\n",
    "\n",
    "The most useful tool that I want to teach you is the function `np.einsum()`, but before we get there let's learn about tensors and axes. Tensors are generalizations of vectors and matrices. For us a tensor is a collection of numbers, where each number is identified by a set of integer coordinates. It's also called an \"array\" in `numpy`.\n",
    "\n",
    "The meaning that we attribute to the array depends on what we will use it for. Here's a couple examples:\n",
    "\n",
    "1. A complex tensor with a single index of dimension $n$ can be interpreted as a vector $T \\in \\mathbb{C}^n$.\n",
    "\n",
    "2. A complex tensor with two indices of dimension $m$ and $n$ can be interpreted as a matrix, i.e. a map $T : \\mathbb{C}^n\\rightarrow \\mathbb{C}^m$, but also as a vector $T \\in \\mathbb{C}^m\\otimes \\mathbb{C}^n$.\n",
    "\n",
    "A general tensor is simply an object with multiple indices: $T_{ijklmn\\dots}$, whose meaning depends on the context.\n",
    "\n",
    "The number of indices is called the _rank_ of the tensor, so column vectors are rank-1 tensors, matrices are rank-2 tensors etc...\n",
    "\n",
    "Each index has a dimension (i.e. the number of distinct integer values that it can have), and the dimension does not have to be the same for all the indices of a tensor, e.g. a rectangular matrix has two indices of different dimensions. If we call $d(j)$ the dimension of the index $j$, then a tensor $T_{j_1,\\dots,j_r}$ of rank $r$ contains $d(j_1)\\times d(j_2)\\times\\dots\\times d(j_r)$ numbers, and so the size of a tensor scales more or less exponentially with the number of indices, i.e. with the rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` has many useful functions to deal with vectors and matrices, but it can also easily deal with higher-order tensors. Here are a few useful functions and methods to know:\n",
    "\n",
    "- `np.newaxis`: this method allows us to create new indices (of dimension 1) for a tensor. Notice that adding a new index of dimension 1 does not change how many numbers are in the tensor (if it were of dimension > 1, then the size of the tensor would have to change).\n",
    "- `Ellipsis` (three dots): this object is a placeholder for any number of indices. Very useful when we want to index a tensor with a variable number of indices.\n",
    "- Broadcasting: this automatic operation allows us to define computations between tensors of seemingly \"incompatible\" shape and save a few keystrokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of np.newaxis\n",
    "\n",
    "v = np.array([1,2,3]) # a vector\n",
    "\n",
    "v1 = v[:, np.newaxis]\n",
    "print(f'Adding a second index. New shape of the tensor is {v1.shape}:\\n', v1, '\\n')\n",
    "\n",
    "v2 = v[np.newaxis, :]\n",
    "print(f'Adding a new first index. New shape of the tensor is {v2.shape}:\\n', v2, '\\n')\n",
    "\n",
    "v3 = v[:, np.newaxis, np.newaxis]\n",
    "print(f'Adding a second and third index. New shape of the tensor is {v3.shape}:\\n', v3, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new axes for broadcasting\n",
    "\n",
    "v = np.array([1,2,3]) # a 3-dim vector\n",
    "m = np.ones((3,3)) # a 3x3 matrix of ones\n",
    "\n",
    "# compare:\n",
    "\n",
    "print('broadcasting along 2nd index (rows):')\n",
    "print(v[np.newaxis, :] * m, '\\n')\n",
    "\n",
    "print('broadcasting along 1st index (columns):')\n",
    "print(v[:, np.newaxis] * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of ellipsis\n",
    "\n",
    "T = np.zeros((2,3,4,5)) # tensor of shape (2,3,4,5)\n",
    "T[..., np.newaxis].shape  # ... represents (2,3,4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 `np.einsum()`\n",
    "Now we can talk about `np.einsum()`, which is an extremely useful function. It can handle all sorts of tensor products, transposition, traces and much more.\n",
    "\n",
    "It takes as first argument a string that explains what happens to the indices (indicated as letters) and then it takes as many tensors as required by the string. There is only one rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{Repeated\\ indices\\ are\\ summed\\ over}\n",
    "$$\n",
    "\n",
    "This means that if we use the same symbol (the same letter) for an index, we mean that in the expression there should be a summation over that index. Let's see a few examples.\n",
    "\n",
    "### Matrix multiplication\n",
    "Matrix multiplication is:\n",
    "\n",
    "$$\n",
    "(MN)_{ik} = \\sum_j M_{ij}N_{jk} =: M_{ij}N_{jk}\n",
    "$$\n",
    "\n",
    "In the last step we use what is called \"Einstein's summation convention\", where we omit writing the summation symbol $\\sum_j$, because $j$ is repeated (it appears in both $M$ and $N$) and therefore it's being summed over. When we sum over a repeated index we say that we \"_contract\" that index_.\n",
    "\n",
    "With `np.einsum` matrix multiplication would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk -> ik', M, N)\n",
    "```\n",
    "\n",
    "Matrix multiplication between, say, 4 matrices is\n",
    "$$\n",
    "(MNPQ)_{im} = \\sum_{jkl} M_{ij}N_{jk}P_{kl}Q_{lm} \\equiv M_{ij}N_{jk}P_{kl}Q_{lm}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk,kl,lm -> im', M, N, P, Q)\n",
    "```\n",
    "\n",
    "### Higher order contractions\n",
    "This can obviously work for tensors of any order! Here's a random example that does not mean anything:\n",
    "\n",
    "$$\n",
    "T_{m} = \\sum_{jkl} M_{j}N_{jklm}P_{jk}Q_{l} \\equiv M_{j}N_{jklm}P_{jk}Q_{l}\n",
    "$$\n",
    "\n",
    "(note that $m$ is the only index here which is never repeated, so the final result must be a vector indexed by $m$)\n",
    "\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('j,jklm,jk,l -> m', M, N, P, Q)\n",
    "```\n",
    "\n",
    "### Transposition\n",
    "The string after the arrow allows us to do some final rearranging of the indices, like a transposition:\n",
    "\n",
    "$$\n",
    "(MN)^T_{ki} = (MN)_{ik}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk -> ki', M, N) # notice: ki and not ik\n",
    "```\n",
    "\n",
    "### Traces\n",
    "If we repeat an index belonging to the same tensor, we compute a trace:\n",
    "\n",
    "$$\n",
    "Tr(M) = \\sum_{i} M_{ii} \\equiv M_{ii}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ii', M)\n",
    "```\n",
    "\n",
    "### Tensor products (i.e. outer products)\n",
    "Outer products are the opposite of inner products, i.e. we simply juxtapose indices:\n",
    "\n",
    "E.g. with vectors:\n",
    "$$\n",
    "\\mathbf{v}\\otimes \\mathbf{w} = v_{i}w_{j}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('i,j -> ij', v, w)\n",
    "```\n",
    "\n",
    "Or with matrices:\n",
    "$$\n",
    "M\\otimes N = M_{ij}N_{kl}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,kl -> ijkl', M, N)\n",
    "```\n",
    "\n",
    "Usually it is a bad idea to \"actually\" calculate an outer product. For better performance keep the parts separated unless the values of the outer product are needed. Chances are that they might get simpler by contracting with other tensors, as the calculation progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2: A better Hilbert-Schmidt inner product\n",
    "\n",
    "The Hilbert-Schmidt inner product is an inner product between complex matrices and it is defined as follows:\n",
    "\n",
    "$$\n",
    "\\langle M, N\\rangle = Tr(M^\\dagger N)\n",
    "$$\n",
    "\n",
    "where $M^\\dagger$ means we transpose and complex-conjugate $M$.\n",
    "\n",
    "- `v1`: implement the formula as is written above, using `np.matmul`, `np.trace` and `np.conj` to compute the conjugate\n",
    "- `v2`: implement the formula using `np.einsum` and `np.conj`\n",
    "- `v3` (Bonus): implement the formula in a more efficient way than `v1` without using `np.einsum` (tip: it looks the same as the function `inner_prod` that you wrote in Activity 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "def HS_inner_product(M, N):\n",
    "    return np.trace(np.matmul(np.transpose(np.conj(M)), N))\n",
    "\n",
    "# v2\n",
    "def HS_inner_product(M, N):\n",
    "    return np.einsum('ji,ji', np.conj(M), N)\n",
    "\n",
    "# v3\n",
    "def HS_inner_product(M, N):\n",
    "    return np.sum(np.conj(M) * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how numpy's syntax allows us to write the inner product in a way that can apply to tensors of any (matching) shape but still look like the formula for the inner product between two vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3: A better way to multiply by a diagonal matrix\n",
    "\n",
    "Consider the product between three matrices: $ABC$, where $B$ is a diagonal matrix. This happens all the time when we consider eigenvalue decomposition or a singular value decomposition, where the central matrix is diagonal.\n",
    "\n",
    "- `v1`: implement this product as written above (treat $B$ as if it were any matrix) using `np.einsum`\n",
    "- `v2`: implement this product in a more efficient way (use the fact that $B$ is diagonal) using `np.einsum` \n",
    "- `v3`: implement this product with `np.einsum` and `np.diag(B)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "def prod(A, B, C):\n",
    "    return np.einsum('ij,jk,kl -> il', A, B, C) # 3 matrices\n",
    "# v2\n",
    "def prod(A, B, C):\n",
    "    return np.einsum('ij,jj,jl -> il', A, B, C) # B is diagonal\n",
    "# v3\n",
    "def prod(A, B, C):\n",
    "    return np.einsum('ij,j,jl -> il', A, np.diag(B), C) # only using the diagonal of B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
