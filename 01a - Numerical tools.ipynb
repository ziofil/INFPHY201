{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png) Filippo Miatto (2024) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: intro and tools\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### After going through this lecture you will be able to:\n",
    "1. Describe the mathematical representation of quantum states\n",
    "2. Use some advanced tensor tools from `numpy`\n",
    "3. Visualize qubits on the Bloch sphere\n",
    "4. Reason about wave functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Summary of Lecture 01***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A bit of useful numpy\n",
    "\n",
    "`Numpy` is one of the most popular python libraries for numerical math.\n",
    "\n",
    "The most useful tool that I want to teach you is the function `np.einsum()`, but before we get there let's learn about arrays and axes. An array is a generalization of vectors and matrices. For us an array is a collection of numbers, where each number is identified by a set of integer coordinates.\n",
    "\n",
    "The meaning that we attribute to the array depends on what we will use it for. Here's a couple examples:\n",
    "\n",
    "1. A complex array with shape `(n,)` can be interpreted as a vector $v \\in \\mathbb{C}^n$.\n",
    "\n",
    "2. A complex array with shape `(m,n)` can be interpreted as a rectangular matrix, i.e. a map $M : \\mathbb{C}^n\\rightarrow \\mathbb{C}^m$, but it can also be interpreted as a vector $v \\in \\mathbb{C}^m\\otimes \\mathbb{C}^n$.\n",
    "\n",
    "A generic array is simply a collection of numbers with multiple indices: $T_{ijklmn\\dots}$, whose meaning depends on the context.\n",
    "\n",
    "The number of indices is called the _order_ of the array, so column vectors are order-1 arrays, matrices are order-2 arrays etc...\n",
    "\n",
    "Each index has a dimension (i.e. the number of distinct integer values that it can have), and the dimension does not have to be the same for all the indices, e.g. a rectangular matrix has two indices of different dimensions. If we call $d(j)$ the dimension of index $j$, then an array $T_{j_1,\\dots,j_r}$ of order $r$ contains $d(j_1)\\times d(j_2)\\times\\dots\\times d(j_r)$ numbers, and so the total size of an array scales more or less exponentially with the number of indices, i.e. with the order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` has many useful functions to deal with vectors and matrices, but it can also easily deal with higher-order arrays. Here are a few useful concepts:\n",
    "\n",
    "- `np.newaxis` (also `None`): this allows us to create new indices (of dimension 1) for an array. Notice that adding a new index of dimension 1 does not change how many numbers are in the array.\n",
    "- `Ellipsis` (three dots): this object is a placeholder for any number of indices. It's very useful when we want to index a tensor with a variable number of indices.\n",
    "- `slice`: slicing an array allows us to access parts of it without creating copies. Numpy is clever enough to gives us a \"view\" into the array and show us only what we ask for, without creating a new object with copies of the content.\n",
    "- Broadcasting: this automatic operation allows us to define computations between tensors of seemingly \"incompatible\" shape and save a few keystrokes.\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.11.10)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3]).flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of np.newaxis\n",
    "\n",
    "v = np.array([1,2,3]) # a vector\n",
    "\n",
    "v1 = v[:, np.newaxis]\n",
    "print(f'Adding a second index. New shape of the tensor is {v1.shape}:\\n', v1, '\\n')\n",
    "\n",
    "v2 = v[np.newaxis, :]\n",
    "print(f'Adding a new first index. New shape of the tensor is {v2.shape}:\\n', v2, '\\n')\n",
    "\n",
    "v3 = v[:, np.newaxis, np.newaxis]\n",
    "print(f'Adding a second and third index. New shape of the tensor is {v3.shape}:\\n', v3, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new axes for broadcasting\n",
    "\n",
    "v = np.array([1,2,3]) # a 3-dim vector\n",
    "m = np.ones((3,3)) # a 3x3 matrix of ones\n",
    "\n",
    "# compare:\n",
    "\n",
    "print('broadcasting along 2nd index (rows):')\n",
    "print(v[np.newaxis, :] * m, '\\n')\n",
    "\n",
    "print('broadcasting along 1st index (columns):')\n",
    "print(v[:, np.newaxis] * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of ellipsis\n",
    "\n",
    "T = np.zeros((2,3,4,5)) # tensor of shape (2,3,4,5)\n",
    "T[..., np.newaxis].shape  # ... represents (2,3,4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 `np.einsum()`\n",
    "Now we can talk about `np.einsum()`, which is an extremely useful function. It can handle all sorts of tensor products, transposition, traces and much more.\n",
    "\n",
    "It takes as first argument a string that explains what happens to the indices (indicated as letters) and then it takes as many tensors as required by the string. There is only one rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{Repeated\\ indices\\ are\\ summed\\ over}\n",
    "$$\n",
    "\n",
    "This means that if we use the same symbol (the same letter) for an index, we mean that in the expression there should be a summation over that index. Let's see a few examples.\n",
    "\n",
    "### Matrix multiplication\n",
    "Matrix multiplication is:\n",
    "\n",
    "$$\n",
    "(MN)_{ik} = \\sum_j M_{ij}N_{jk} =: M_{ij}N_{jk}\n",
    "$$\n",
    "\n",
    "In the last step we use what is called \"Einstein's summation convention\", where we omit writing the summation symbol $\\sum_j$, because $j$ is repeated (it appears in both $M$ and $N$) and therefore it's being summed over. When we sum over a repeated index we say that we \"_contract\" that index_.\n",
    "\n",
    "With `np.einsum` matrix multiplication would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk -> ik', M, N)\n",
    "```\n",
    "\n",
    "Matrix multiplication between, say, 4 matrices is\n",
    "$$\n",
    "(MNPQ)_{im} = \\sum_{jkl} M_{ij}N_{jk}P_{kl}Q_{lm} \\equiv M_{ij}N_{jk}P_{kl}Q_{lm}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk,kl,lm -> im', M, N, P, Q)\n",
    "```\n",
    "\n",
    "### Higher order contractions\n",
    "This can obviously work for tensors of any order! Here's a random example that does not mean anything:\n",
    "\n",
    "$$\n",
    "T_{m} = \\sum_{jkl} M_{j}N_{jklm}P_{jk}Q_{l} \\equiv M_{j}N_{jklm}P_{jk}Q_{l}\n",
    "$$\n",
    "\n",
    "(note that $m$ is the only index here which is never repeated, so the final result must be a vector indexed by $m$)\n",
    "\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('j,jklm,jk,l -> m', M, N, P, Q)\n",
    "```\n",
    "\n",
    "### Transposition\n",
    "The string after the arrow allows us to do some final rearranging of the indices, like a transposition:\n",
    "\n",
    "$$\n",
    "(MN)^T_{ki} = (MN)_{ik}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk -> ki', M, N) # notice: ki and not ik\n",
    "```\n",
    "\n",
    "### Traces\n",
    "If we repeat an index belonging to the same tensor, we compute a trace:\n",
    "\n",
    "$$\n",
    "Tr(M) = \\sum_{i} M_{ii} \\equiv M_{ii}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ii', M)\n",
    "```\n",
    "\n",
    "### Tensor products (i.e. outer products)\n",
    "Outer products are the opposite of inner products, i.e. we simply juxtapose indices:\n",
    "\n",
    "E.g. with vectors:\n",
    "$$\n",
    "\\mathbf{v}\\otimes \\mathbf{w} = v_{i}w_{j}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('i,j -> ij', v, w)\n",
    "```\n",
    "\n",
    "Or with matrices:\n",
    "$$\n",
    "M\\otimes N = M_{ij}N_{kl}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,kl -> ijkl', M, N)\n",
    "```\n",
    "\n",
    "Usually it is a bad idea to \"actually\" calculate an outer product. For better performance keep the parts separated unless the values of the outer product are needed. Chances are that they might get simpler by contracting with other tensors, as the calculation progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2: A better Hilbert-Schmidt inner product\n",
    "\n",
    "The Hilbert-Schmidt inner product is an inner product between complex matrices and it is defined as follows:\n",
    "\n",
    "$$\n",
    "\\langle M, N\\rangle = Tr(M^\\dagger N)\n",
    "$$\n",
    "\n",
    "where $M^\\dagger$ means we transpose and complex-conjugate $M$.\n",
    "\n",
    "- `v1`: implement the formula as is written above, using `np.matmul`, `np.trace` and `np.conj` to compute the conjugate\n",
    "- `v2`: implement the formula using `np.einsum` and `np.conj`\n",
    "- `v3` (Bonus): implement the formula in a more efficient way than `v1` without using `np.einsum` (tip: it looks the same as the function `inner_prod` that you wrote in Activity 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "def HS_inner_product(M, N):\n",
    "    return np.trace(np.matmul(np.transpose(np.conj(M)), N))\n",
    "\n",
    "# v2\n",
    "def HS_inner_product(M, N):\n",
    "    return np.einsum('ji,ji', np.conj(M), N)\n",
    "\n",
    "# v3\n",
    "def HS_inner_product(M, N):\n",
    "    return np.sum(np.conj(M) * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how numpy's syntax allows us to write the inner product in a way that can apply to tensors of any (matching) shape but still look like the formula for the inner product between two vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3: A better way to multiply by a diagonal matrix\n",
    "\n",
    "Consider the product between three matrices: $ABC$, where $B$ is a diagonal matrix. This happens all the time when we consider eigenvalue decomposition or a singular value decomposition, where the central matrix is diagonal.\n",
    "\n",
    "- `v1`: implement this product as written above (treat $B$ as if it were any matrix) using `np.einsum`\n",
    "- `v2`: implement this product in a more efficient way (use the fact that $B$ is diagonal) using `np.einsum` \n",
    "- `v3`: implement this product with `np.einsum` and `np.diag(B)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "def prod(A, B, C):\n",
    "    return np.einsum('ij,jk,kl -> il', A, B, C) # 3 matrices\n",
    "# v2\n",
    "def prod(A, B, C):\n",
    "    return np.einsum('ij,jj,jl -> il', A, B, C) # B is diagonal\n",
    "# v3\n",
    "def prod(A, B, C):\n",
    "    return np.einsum('ij,j,jl -> il', A, np.diag(B), C) # only using the diagonal of B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
