{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time evolution\n",
    "We now turn our attention to larger systems and to time evolution. Remember last week, when we diagonalized the Hamiltonian of a particle in a box? Today we will see where that comes from, as well as a bit on the SchrÃ¶dinger equation.\n",
    "\n",
    "### 4.1 How to write a Hamiltonian\n",
    "Writing a Hamiltonian is an important thing to know. Once we have the Hamiltonian, we can compute the time evolution operator and with that we can fast-forward time and rewind the clock on any quantum state!\n",
    "\n",
    "We need to begin from the role of the Hamiltonian as the Observable of energy. Energy in mechanical systems (like a particle in a box) comes in two forms: kinetic and potential: $H = H_\\mathrm{kinetic} - H_\\mathrm{potential}$. Classically, the kinetic energy can be written as\n",
    "\n",
    "$$\n",
    "E_\\mathrm{kinetic} = \\frac{P^2}{2m}\n",
    "$$\n",
    "\n",
    "Where $P$ is the momentum and $m$ is the mass of the particle (which is just a numerical parameter). So in order to write the kinetic contribution to the Hamiltonian, we need the momentum Observable. What is the momentum observable? To answer this question we need to go back for a second to the fact that position and momentum are continuous quantities, and their Observable are actually defined in an infinite-dimensional Hilbert space. This is very convenient because in an infinite-dimensional Hilbert space we can do calculus (integrals, derivatives and so on). In fact, those are our only tools to define operators (which in that case cannot be matrices, because the space is infinite-dimensional).\n",
    "\n",
    "So what we'll do is we will find the momentum Observable $P$ with a trick. First of all, we'll need something that contains $P$. Luckily we know that the unitary operator that shifts the position of a particle is generated by the momentum Observable:\n",
    "\n",
    "$$\n",
    "U(x) = \\exp(i x P)\n",
    "$$\n",
    "\n",
    "So we can write:\n",
    "\n",
    "$$\n",
    "-i\\frac{\\partial}{\\partial x}U(x) = P\\exp(ixP) = PU(x)\n",
    "$$\n",
    "\n",
    "Which means that:\n",
    "\n",
    "$$\n",
    "-i\\frac{\\partial}{\\partial x} = P\n",
    "$$\n",
    "\n",
    "and therefore \n",
    "$$\n",
    "P^2 = -\\frac{\\partial^2}{\\partial x^2}\n",
    "$$\n",
    "\n",
    "This makes perfect sense in an infinite-dimensional vector space, because the operators on it are in differential form (they are not matrices). But what happens when we discretize the space? We saw that integrals become sums, what about derivatives? This is simpler than one may think: a derivative is the limit for $\\delta x\\rightarrow 0$ of $$\\frac{\\psi(x + \\delta x) - \\psi(x)}{\\delta x}$$ In a discrete space though, we can't have $\\delta x$ go all the way to zero! It will have to shrink at most to the size of the smallest interval. But then $\\psi(x+\\delta x)$ and $\\psi(x)$ are two amplitudes that are next to each other in the discrete space. So that tells us how to construct the matrix the implements the discrete derivative: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} = \n",
    "\\begin{pmatrix}\n",
    "-1 & 1 & 0 & 0 & \\cdots\\\\\n",
    "0 & -1 & 1 & 0 &\\cdots\\\\\n",
    "0 & 0 & -1 & 1 & \\cdots\\\\\n",
    "\\vdots & \\vdots & \\vdots& \\ddots & \\ddots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This is a well-known technique in finite element analysis. Okay, we are almost done! To obtain the second derivative we can apply the formula for the (symmetric) second derivative $\\frac{1}{\\delta x^2}(\\psi(x-\\delta x) - 2\\psi(x) + \\psi(x+\\delta x))$ and we obtain the matrix that implements it:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x^2} = \n",
    "\\begin{pmatrix}\n",
    "-2 & 1 & 0 & 0 & \\cdots\\\\\n",
    "1 & -2 & 1 & 0 &\\cdots\\\\\n",
    "0 & 1 & -2 & 1 & \\cdots\\\\\n",
    "0 & 0 & 1 & -2 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots& \\ddots & \\ddots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "With this, we can finally build the symmetric Hamiltonian:\n",
    "\n",
    "$$\n",
    "H = \\frac{1}{2m}\n",
    "\\begin{pmatrix}\n",
    "2 & -1 & 0 & 0 & \\cdots\\\\\n",
    "-1 & 2 & -1 & 0 &\\cdots\\\\\n",
    "0 & -1 & 2 & -1 & \\cdots\\\\\n",
    "0 & 0 & -1 & 2 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\ddots& \\ddots & \\ddots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And that tells us how to construct the kinetic term of the Hamiltonian! The potential term is easy (especially in our case). If we have a potential that depends on the position of the particle (like a potential well, or a harmonic oscillator like a mass on a spring etc...) then it will be diagonal in the position basis. It will simply be a diagonal matrix with the value of the potential $V(x)$ at the discrete position $x$. A box with solid boundaries has zero potential inside and infinite potential outside. So for us it's trivial to implement the walls of the box: they correspond to the first and last index! Our quantum state is guaranteed to stay in the box because there are no indices to describe positions outside of it! So here's our Hamiltonian:\n",
    "\n",
    "Finally, because diagonalization algorithms can get picky when things are not symmetric, we will shift\n",
    "\n",
    "---\n",
    "#### Activity 4: Implement the Hamiltonian (10 minutes)\n",
    "Write a function that takes the mass of the particle and the dimension of the Hilbert space and returns the matrix of the Hamiltonian. The signature should be `f(float, int) -> np.array(complex)`. TIP: the function `np.diag()` has a second argument that you can use to fill the diagonal of your choice, not just the central one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(m, dim):\n",
    "    P_squared = np.diag(2*np.ones(dim), k=0) + np.diag(-1*np.ones(dim-1), k=1) + np.diag(-1*np.ones(dim-1), k=-1)\n",
    "    return P_squared/(2*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 How to compute the action of the time evolution operator\n",
    "We are going to see two ways of computing time evolution.\n",
    "\n",
    "The first way to compute the action of $U(t)$ on a state is to exponentiate the Hamiltonian for a fixed choice of time interval $\\Delta t$, and the resulting matrix advances time by $\\Delta t$ every time we use it to multiply the state:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U(\\Delta t)|\\psi(0)\\rangle &= |\\psi(\\Delta t)\\rangle\\\\\n",
    "U(\\Delta t)|\\psi(\\Delta t)\\rangle &= |\\psi(2\\Delta t)\\rangle\\\\\n",
    "\\mathrm{etc}\\dots\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we need to compute $U(\\Delta t)$ only once and for all.\n",
    "\n",
    "If we want more flexibility, or for example to have $|\\psi(t)\\rangle$ for all times $t$ without having to recompute a new matrix exponential for all $t$, we can adopt the second way. We express $|\\psi(0)\\rangle$ in the eigenbasis of $H$ and then we multiply the $k$-th amplitude by $\\exp(it\\lambda_k)$. This is a much simpler calculation than a whole matrix exponential (so it's much faster) and it gives us $|\\psi(t)\\rangle$ directly.\n",
    "\n",
    "Let's implement both. We start by computing $U(\\Delta t)$ for a small time interval, say $\\Delta t=1$ (this is a small interval in relation to the energy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = expm(1j*1.0*H(0.5, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = np.array([np.exp(-(x-100)**2/(2*25)) for x in range(200)])\n",
    "psi = psi/np.linalg.norm(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = 100\n",
    "state = psi\n",
    "for k in range(100):\n",
    "    plt.plot(abs(state)**2)\n",
    "    state = U@state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've written a function (very inefficient as it creates a new figure object each time) to create a \"live plot\". If you find a way to make it faster (for instance by reusing the same figure object) let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data = defaultdict(list)\n",
    "\n",
    "psi = np.array([np.exp(-(x-100)**2/(2*25)) for x in range(200)])\n",
    "speed = 100\n",
    "state = psi*np.exp(-1j*speed*np.linspace(-1,1,200))\n",
    "\n",
    "for i in range(100):\n",
    "    state = U@state\n",
    "    data['prob']= np.abs(state)**2\n",
    "    live_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the second method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, Vdagger = np.linalg.eig(H(0.5, 200))\n",
    "\n",
    "psi = np.array([np.exp(-(x-100)**2/(2*25)) for x in range(200)])\n",
    "\n",
    "# the wave function in the eigenbasis of H\n",
    "psi_H = np.conj(Vdagger.T)@psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_evolved(t):\n",
    "    return Vdagger@(np.exp(1j*eigenvalues*t)*psi_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "\n",
    "for t in range(100):\n",
    "    data['prob']= np.abs(psi_evolved(t))**2\n",
    "    live_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
